{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-9b81220fb3f7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnode2vec\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mNode2Vec\u001b[0m  \u001b[1;31m# NODE2VEC EMBEDDINGS\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mclassifier\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mclf\u001b[0m  \u001b[1;31m# VERTEX CLASSIFIER\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import utils  \n",
    "from node2vec import Node2Vec  # NODE2VEC EMBEDDINGS \n",
    "import classifier as clf  # VERTEX CLASSIFIER\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DOMAIN OF THE DATA\n",
    "\n",
    "PPI dataset can be model as a graph of 44906 vertexes belong to the training set (split in 20 sub-graphs) and 12038 belong to the test set (split in 4 sub-graphs) each one charecterized by a binary features vectort (belong to (0,1)^50) very sparse and a no fixed number of undirected edges that connect each vertex to the others. We know the labels (belong to (0,1)^122) of the training set and we want to predict them in the test set with the best accuracy (compute by F1 score). \n",
    "These are the distributions of the data: \n",
    "\n",
    "<img src=\"FeaturesTES.png\" style=\"height:300px;width400px\">\n",
    "<img src=\"FeaturesTEScardinality.png\" style=\"height:300px;width400px\">\n",
    "<img src=\"FeaturesTRS.png\" style=\"height:300px;width400px\">\n",
    "<img src=\"FeaturesTEScardinality.png\" style=\"height:300px;width400px\">\n",
    "<img src=\"FeaturesTES.png\" style=\"height:300px;width400px\">\n",
    "<img src=\"LabelsTRScardinality.png\" style=\"height:300px;width400px\">\n",
    "\n",
    "(sono le 6 immagini delle ditribuzioni dei dati, da visualizzare a 2 per riga)\n",
    "\n",
    "'Feature10' is costant to zero in each vertex of the dataset, so we delate it because it can’t explain any causal correlation with the labels. More in general the sparse nature of the features show us that to be able to extract attributes from the graph's structural information is essential in this dataset. Per quanto riguarda le labels invece, we can assume that, for the (naive) indipendence between the Nodes and for Central Limit Theorem, the same means of elements belong to each classes si ripresenterà nel test set (questo ci sarà utile più avanti)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOADING THE DATA\n",
    "vertices_train, vertices_test = utils.build_vertices()\n",
    "\n",
    "X_train_df = utils.build_dataframe(vertices_train, \"feature\")\n",
    "X_train_df = X_train_df.drop(['feature_10'], axis=1)  # DROP USELESS FEATURE\n",
    "\n",
    "X_test_df = utils.build_dataframe(vertices_test, \"feature\")\n",
    "X_test_df = X_test_df.drop(['feature_10'], axis=1)  # DROP USELESS FEATURE\n",
    "\n",
    "labels_df = utils.build_dataframe(vertices_train, \"label\", preserve_int_col_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOADING\n",
    "\n",
    "Loaded the data, we build the graph 'G' to model the protein-protein interactions and we extract the matrixes of features ‘X’  (belong to (0,1)^n*50) and the matrix of labels ‘labels’ (belong to (0,1)^n*50)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUILDING NUMPY MATRICES \n",
    "X_train = X_train_df.values\n",
    "X_test = X_test_df.values\n",
    "X = np.concatenate((X_train, X_test), axis=0)\n",
    "labels = labels_df.values\n",
    "\n",
    "\n",
    "# MASK FOR TRAIN/TEST\n",
    "train_idx = range(X_train.shape[0])\n",
    "test_idx = range(X_train.shape[0],X.shape[0])\n",
    "\n",
    "# BUILDING THE GRAPH\n",
    "G = utils.build_graph()\n",
    "G = G.to_directed()  # CONSIDERING THE GRAPH AS UNDIRECTED\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EMBEDDING\n",
    "\n",
    "Any supervised machine learning algorithm requires a set of informative, discriminating, and independent features. In multi-label vertex classification on graphs this means that one has to construct a feature vector representation for each node, called embedding, but there is no straightforward way to encode this high-dimensional, non-Euclidean information into a low-dimensional vector.\n",
    "A typical solution involves hand-engineering domain-specific features based on expert knowledge but it is onerous and not generalizable. \n",
    "An alternative approach is to learn feature representations by solving an optimization problem. The challenge in feature learning is defining an objective function balancing computational efficiency and predictive accuracy. The goal is to optimize this objective function so that geometric relationships in the embedding space reflect the structure of the original graph. \n",
    "In particular we assume that two nodes are neighbour in the embedding space if their features are similar, they belong to the same communities (homophily assumpion) and share similar roles (stuctural equivalence assumption). \n",
    "For instance, in Figure 1, we observe nodes u and s1 belonging to the same tightly knit community of nodes, while the nodes u and s6 in the two distinct communities share the same structural role of a hub node.\n",
    "\n",
    "(foto del paper node2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NODE2VEC\n",
    "\n",
    "In our first approach to the problem we don't consider the matrixes of features and we focus to embed the vertexes through node2vec algorithm with a definition of neighborhood as flexible as possible. Node2vec indeed, is a semi-supervised algorithm designed to natural lenguage processing for scalable feature learning that instead of trying to decode a\n",
    "deterministic node similarity measure, it optimizes embeddings encoding the statistics of random walks.\n",
    "For every source node u ∈ V , we define NS(u) ⊂ V as a network neighborhood of node u generated through a neighborhood sampling strategy S.\n",
    "Given a source node u, we simulate a random walk of fixed length l. Let ci denote the ith node in the walk, starting with c0 = u. Nodes ci are generated by the following distribution: (formula 3.2.1 paper)\n",
    "where πvx is the unnormalized transition probability between nodes v and x, and Z is the normalizing constant. We set then the unnormalized transition probability to πvx = αpq(t, x), where (formula). The use of two parameters 'p' and 'q' allowed to combine in a single path breadth-first sampling and depth-first sampling. In particular the return parameter 'p' controls the likelihood of immediately reviseting a node in the walk, and the in-out parameters 'q' allowes the search to differentiate between \"inward\" and \"outward\" nodes. Here we set p=1 and q=5 to bias the walk close to node t (BFS) to emphasize structural equivalence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|██████████| 56944/56944 [10:56<00:00, 86.75it/s] \n",
      "Generating walks (CPU: 1): 100%|██████████| 10/10 [04:35<00:00, 29.62s/it]\n"
     ]
    }
   ],
   "source": [
    "# GENERATING WALKS OVER THE GRAPH\n",
    "node2vec = Node2Vec(G, dimensions=128, walk_length=12, num_walks=10, p=1, q=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SKIP-GRAM (da rivedere) (mappi in maniera diversa contest e oggetto?)\n",
    "\n",
    "We proceed by extending the Skip-gram architecture for natural lenguage processing to the graph. We seek to optimize the following objective function, which maximizes the log-probability of observing a network neighborhood NS(u) for a node u conditioned on its feature representation, given by f:\n",
    "(formula)\n",
    "\n",
    "In order to make the optimization problem tractable, we make two standard assumptions:\n",
    "• Conditional independence. We factorize the likelihood by assuming that the likelihood of observing a neighborhood node is independent of observing any other neighborhood node given the embedding representation of the source:\n",
    "(formula)\n",
    "\n",
    "\n",
    "(verifica)\n",
    "• Symmetry in feature space. A source node and neighborhood node have a symmetric effect over each other in feature space. Accordingly, we model the conditional likelihood of every source-neighborhood node pair as a softmax unit parametrized by a dot product of their embeddings:\n",
    "(formula)\n",
    "\n",
    "With the above assumptions, the objective in Eq. 1 simplifies to:\n",
    "(formula)\n",
    "\n",
    "We optimize it using stochastic gradient ascent over the model parameters defining the features f."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# FITTING THE SKIP-GRAM MODEL\n",
    "skipgram = node2vec.fit(window=8)\n",
    "\n",
    "\n",
    "#  SAVING OUR EMBEDDINGS\n",
    "embedding_n2v = skipgram[skipgram.wv.vocab]   \n",
    "np.save('./embedding/embedding_n2v.npy', embedding_n2v)  # SAVING AS  A .NPY FILE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLASSIFICATION \n",
    "\n",
    "Once we have embeded each vertex of the graph the problem becames a standard classification. Because besides we don't use yet the matrixes of features we concatenate them with the embeddings just obtained. In general we should combine only the elements of the orginal features and embeddings with high variance and slow correlation with the others elements. However we assume indipendence between these elements of differens nature and because the feature+embedding space isn't too large also a reduction as PCA is not necessary.\n",
    "\n",
    "To improve the classification instead, through cross validation, we train the model many times subdividing the training set and making the average of the parameters obtained. So we can also compute an accuracy from the training set but it isn't necessary ugual to the test set one because the model can always overfit. \n",
    "(classificazione pesata come?)\n",
    "The last improvement to the classifier is to condider some inner weights to emphasize the wrong classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tHamming accuracy: 0.438\n",
      "\tAccuracy, exact matches: 0.000\n",
      "\tMacro F1 Score: 0.439\n",
      "\tMicro F1 Score: 0.470\n"
     ]
    }
   ],
   "source": [
    "# X MATRIX FOR CLASSIFICATION\n",
    "train_embedding_n2v = embedding_n2v[train_idx]  # EXTRACTING TRAINING SET EMBEDDINGS\n",
    "test_embedding_n2v = embedding_n2v[test_idx]  #  EXTRACTING TEST SET EMBEDDING\n",
    "X_train_n2v = np.concatenate((X_train, train_embedding_n2v), axis=1)  #  FEATURES + EMBEDDING\n",
    "X_test_n2v = np.concatenate((X_test, test_embedding_n2v), axis=1)  # FEATURES + EMBEDDINGS\n",
    "\n",
    "\n",
    "#  TESTING VALIDATION ACCURACY\n",
    "#clf.validation_accuracy(X_train_n2v, labels)\n",
    "\n",
    "#  TESTING TRAIN ACCURACY\n",
    "node2vec_model = clf.fit_model(X_train_n2v, labels)  # RETURNS TRAINED MODEL AND TRAIN ACCURACY\n",
    "\n",
    "\n",
    "#  MAKING PREDICTIONS\n",
    "node2vec_pred = node2vec_model.predict_proba(X_test_n2v) > 0.4  # EVALUATING PREDICTIONS\n",
    "utils.get_results('./results/node2vec_pred.csv',node2vec_pred, X_test_df)  # SAVING RESULTS IN A .CSV FOR KAGGLE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NODE2VEC CONCLUSION\n",
    "\n",
    "The algorithm shows competitive performance for the dataset even if 0.438 of accuracy is not sufficent to be called good classification. In add to the flexible definition of neighborhood other strenghts are that for the stochastic nature it is robust to perturbations in the form of noisy or missing edges. Furthermore computationally, the major phases\n",
    "of node2vec are trivially parallelizable, and it can scale to large graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRAPH CONVOLUTIONAL NETWORK\n",
    "\n",
    "Because above we haven't considered the matrixes of features to build the embeddings, now we try to encode both graph structure and features of nodes through a Graph Convolutional Network, an other semi-supervised algorithm more general. \n",
    "Following the idea of T.N. Kipf and M.Wallington we encode the graph structure directly using a neural network model f(X, A) and train on a supervised target for all nodes with labels, thereby avoiding explicit graph-based regularization in the loss function that need the assumpion that \"connected nodes share similar labels\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from gcn.model import GCN \n",
    "from train  import train_model \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primarily we extract the binary adjacency matrix 'A' from the graph, that is symmetrycal because the edges are undirected. Then, without delate the autolaps, we add to 'A' lambda-times the identity matrix 'I' to highlight the importance of own original features in the embeddigs (sum rule), although the original features are very sparse. We also define matrix of degrees Dtilde as (formula) for the Mean Rule. What we want to do is a spectral convolution on the graph and we use Dtilde to normalize the feature rappresentation. Indeed, a propagation rule without a normalization implies that nodes with large degrees will have large values in their features, while nodes with small degrees will have small values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUILDING ADJ MATRIX FOR GCN\n",
    "A = utils.adjacency_matrix_GCN(G, 1)  \n",
    "\n",
    "\n",
    "# DEFINING OUR PARAMETERS\n",
    "n_features = X.shape[1]\n",
    "n_classes = labels.shape[1]\n",
    "n_hidden = 32  #  NUMBER OF HIDDEN PARAMETERS IN OUR NET\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PROPAGATION RULE\n",
    "\n",
    "We can so define the propagation rule:\n",
    "(formula se possibile con piccola dimostrazione dello sviluppo di chebishev)\n",
    "H(o)=X\n",
    "H(l+1)= ...\n",
    "Fixed the number of layers and their dimensions, through a neaural network we build the embeddings of the nodes (that directly rapresent the labels). With this aim we reasearch the best values of the matrixes of weights 'W' that minimize the cross entropy error, training the model with the backpropagation rule. On the other hand, to optimize the number of layers is an optimization problem too complex, so we set 2 hidden layers of 16 neurons each one in a heurstic way. This means that each the embedding of each node combine the feature of his neighbor untill the second order. Finally, to reduce the large numbers of parameters of the matrixes of weghts and prevent neural network from overfitting we apply Dropout: the key idea is to randomly drop units, along with their connections, from the neural network during training. \n",
    "(immagine dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:20<00:00,  2.13s/it]\n"
     ]
    }
   ],
   "source": [
    "# CREATING AND TRAINING OUR MODEL\n",
    "\n",
    "gcn_model = GCN(n_features, n_hidden, n_classes, dropout=0.5)\n",
    "embedding_gcn = train_model(gcn_model, X, A, labels, train_idx, epochs=50, lr=0.005, wd=5e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tHamming accuracy: 0.710\n",
      "\tAccuracy, exact matches: 0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tMacro F1 Score: 0.195\n",
      "\tMicro F1 Score: 0.506\n"
     ]
    }
   ],
   "source": [
    "#  EXTRACTING EMBEDDINGS \n",
    "train_embedding_gcn = embedding_gcn[train_idx]\n",
    "test_embedding_gcn = embedding_gcn[test_idx]\n",
    "\n",
    "\n",
    "#  TESTING TRAIN ACCURACY (SIGMOID TRESHOLD SET AT 0.4 )\n",
    "gcn_train_pred = torch.sigmoid(train_embedding_gcn).detach().numpy() > 0.4 \n",
    "utils.get_score(gcn_train_pred, labels)  # HAMMING ACCURACY, F1-MICRO, F1-MACRO\n",
    "\n",
    "#  SAVING THE EMBEDDING\n",
    "np.save('./embedding/embedding_gcn.npy', embedding_gcn.detach().numpy())  # .NPY FILE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  MAKING OUR PREDICTION\n",
    "gcn_test_pred = torch.sigmoid(test_embedding_gcn).detach().numpy() > 0.5  # PREDICTIONS ON TEST SET\n",
    "gcn_test_pred = utils.a_third_law(labels,gcn_test_pred) # 0.475 accuracy\n",
    "utils.get_results('./results/gcn_pred.csv',gcn_test_pred, X_test_df)  # SAVING RESULTS IN A .CSV \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given each embedding belong to R^122 we map it in (0,1)^122 through the sigmoid function and activate it to obtain label prediction. How we could expected, for the generality and structural complexity of the model the accuracy increase. However this increase is minimal and the score is not sufficient yet to be called good classification.\n",
    "At this point our suspect increase: Are our sparse features belong to (0,1)^50 adequately rappresentative to this task of multilabel vertex classification?\n",
    "Before to give an answer we try to implement also GraphSAGE and GAT, others two state-of-the-art algorithms to this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GraphSAGE\n",
    "\n",
    "Aiming to make our model richer and richer, from the idea of GCN, we develop now GraphSAGE, a general inductive framework that leverages node feature information to efficiently generate node embeddings for previously unseen data. \n",
    "Although inductive capability isn't necessary for our problem we highlight that it is essential for high-throughput and production machine learning systems, which operate on evolving graphs and constantly encounter unseen nodes. In particular could became really usefull if our dataset of PPI were continuosly modified. So, instead of training individual embeddings for each node, it learns a function that generates embeddings by sampling and aggregating features from a node’s local neighborhood. Furthermore, as GCN, by incorporating node features in the learning algorithm, it simultaneously learns the topological structure of each node’s neighborhood as well as the distribution of node features in the neighborhood. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sage.model import SAGE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intuition behind GraphSAGE is that at each iteration, nodes aggregate information from their local neighbors, and as this process iterates, nodes incrementally gain more and more information from further reaches of the graph.\n",
    "First, each node v ∈ V aggregates the representations of the nodes in its\n",
    "immediate neighborhood, into a single vector. This aggregation step depends on the representations generated at the previous iteration of the outer loop (i.e., k − 1), and the k = 0 (“base case”) representations are defined as the input node features. After aggregating the neighboring feature vectors, GraphSAGE then concatenates the node’s current representation, with the aggregated neighborhood vector, and this concatenated vector is fed through a fully connected layer with nonlinear activation function σ, which transforms the representations to be used at the next step of the algorithm. We adopt the elementwise mean aggreator as e aggregator function.\n",
    "\n",
    "This Pseudo-code resume and order all the passes of GraphSAGE\n",
    "(immagine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [04:31<00:00, 10.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tHamming accuracy: 0.306\n",
      "\tAccuracy, exact matches: 0.000\n",
      "\tMacro F1 Score: 0.445\n",
      "\tMicro F1 Score: 0.468\n"
     ]
    }
   ],
   "source": [
    "#  CREATING THE MODEL\n",
    "edge_list = torch.LongTensor(utils.edge_list_SAGE())  # list of all the edges\n",
    "sage_model = SAGE(n_features, n_hidden, n_classes, 0.5)\n",
    "\n",
    "\n",
    "#  TRAINING THE MODEL \n",
    "embedding_sage = train_model(sage_model, X, edge_list, labels, train_idx, epochs=25, lr=0.1,wd=5e-3)\n",
    "sage_train_pred = torch.sigmoid(embedding_sage[train_idx]).detach().numpy() > 0.5# TRESHOLD ON SIGMOID PROBABILITIES\n",
    "sage_train_pred = utils.a_third_law(labels,sage_test_pred)\n",
    "utils.get_score(sage_train_pred, labels)  # HAMMING ACCURACY, F1-MICRO, F1-MACRO\n",
    "\n",
    "\n",
    "#  SAVING THE EMBEDDING\n",
    "np.save('./embedding/embedding_sage.npy', embedding_sage.detach().numpy())  # .NPY FILE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  MAKING OUR PREDICTION\n",
    "sage_test_pred = torch.sigmoid(embedding_sage[test_idx]).detach().numpy() > 0.5  # PREDICTIONS ON TEST SET\n",
    "sage_test_pred = utils.a_third_law(labels,sage_test_pred) # BEST SAGE 0.477\n",
    "utils.get_results('./results/sage_test_pred.csv',sage_test_pred, X_test_df)  # SAVING RESULTS IN A .CSV \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite the efforts also GraphSAGE performes an accurancy between 0.44 and 0.46 on the test set, virtually identical to the previous alghorithms. There are already too much labels that we aren't able to predict correctly.\n",
    "Before to implent GAT we try to combine the three methods seen untill now to combine the respective strenghts. To avoid to connect an eccessive number of features and extend too much the hypotesis space, in these new embeddings we try to consider only the directions of maximum variance (through PCA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.concatenate((embedding_n2v,embedding_sage),axis=1)  # N2V EMBEDDING concatenated SAGE EMBEDDING\n",
    "clf.validation_accuracy(Y[train_idx],labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to understand the real efficacy of these concatenations we evaluete these new embeddings through a cross validation on the training set because it is more rappresentative than the test set and don't lead us to make considerations only to force to improve the accuracy of the test set. However either GraphSAGE+Node2Vec (around 0.425 of micro), either GCN+Node2Vec get worse the accuracy obained with the singles algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = 64)\n",
    "Y_pca = pca.fit_transform(Y)  #  250 --> 64\n",
    "print('Explained variance: ',np.sum(pca.explained_variance_ratio_))  #  GOOD EXPLAINED VARIANCE\n",
    "clf.validation_accuracy(Y_pca[train_idx],labels)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even the principal compenent analysis, that map these new embeddings of around 250 features in a low-dimensional space that consider only the 64 directions of maximum variance, don't improve our predictions.\n",
    "This isn't a problem of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GAT\n",
    "\n",
    "A last resort for our multi-label vertex classification problem is to implement GAT. We read that it can performe drastically better than each approaches seen untill now if the features are sparse. In particoular we read in this paper (linkare graph attention network) that GAT can performe an accuracy of more than 0.973 in a dataset of protein-protein interactions with the same dimension of th our. We don't understand yet how could be possible but we continue our research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gat.ppi import PPI\n",
    "from gat.model import GAT\n",
    "from gat.training import train,test\n",
    "from torch_geometric.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GAT is a novel neural network architectures that operates on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations.\n",
    "The idea is to compute the hidden representations of each node in the graph, by attending over its neighbors, following a self-attention strategy. The attention architecturehas several interesting properties: the operation is efficient, since it is parallelizable across nodeneighbor pairs; it can be applied to graph nodes having different degrees by specifying arbitrary\n",
    "weights to the neighbors; and, as GraphSAGE, the model is directly applicable to inductive learning problems, including tasks where the model has to generalize to completely unseen graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  LOADING DATASETS \n",
    "train_dataset = PPI('./gat',split='train')\n",
    "test_dataset = PPI('./gat',split='test')\n",
    "valid_dataset = PPI('./gat',split='valid')  #  LET'S KEEP SOME TRAINING SAMPLES FOR VALIDATION..\n",
    "\n",
    "\n",
    "#  BATCH TRAINING.. \n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=2, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Graph Attention Network is composed by many Graph attentional Layers. Each one map a set of node features in a new set of features (potentially of different cardinality). In order to obtain sufficient expressive power to transform the input features into higher-level features, as an initial step, a shared learnable linear transformation, parametrized by a weight matrix, is applied to every node (formula). Then, a shared attentional mechanism 'a' computes attention coefficients that indicate the importance of node j's features to node i. This idea self-attention is a sort genaralization of the GCN algorithm were instead each node has the same influence to its neighbors.  Actually we have already used a sort of attentional coefficent in the GCN lambda when we added lambda-times the identity matrix to the adjacency matrix to emphasize the importance of autolap. \n",
    "To make coefficients easily comparable across different nodes, we normalize them across all choices of j using the softmax function. (formula)\n",
    "Once obtained, the normalized attention coefficients are used to compute a linear combination of the features corresponding to them, to serve as the final output features for every node. (formula)\n",
    "To stabilize the learning process of self-attention, we also extend the mechanism to employ multi-head attention (formula)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the inductive learning task, we apply a three-layer GAT model. Both of the first two layers consist of K = 4 attention heads computing F0 = 256 features, followed by an ELU nonlinearity. The final layer is used for (multi-label) classification:\n",
    "K = 6 attention heads computing 122 features each, that are averaged and followed by a logistic sigmoid activation.\n",
    "Although the algorithm is parellelizable without a GPU our calculators aren't able to train the model. Till 500 minutes are necessary for a single epoch. So we rent a Tesla K80 and we compute all the optimization in only few minutes.<b>\n",
    "Trying to improve even more the accurancy, with our simple statistical considerations we impose that the most probable classes of labels are one for all the test set. Even if this kinds of considerations will never lead us to a perfect prediction but at the leatest to a good esteem, however this addition improve here the accuracy of around 0.02."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  BUILDING AND TRAINING OUR MODEL\n",
    "model = GAT(n_features,n_classes)\n",
    "for epoch in range(1, 60):\n",
    "    loss = train(model,train_loader) \n",
    "    acc = test(model,valid_loader) \n",
    "    print('Epoch: {:02d}, Loss: {:.4f}, Acc: {:.4f}'.format(epoch, loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  MAKING PREDICTIONS\n",
    "model.cpu()\n",
    "for data in test_loader:    \n",
    "    out = model(data.x,data.edge_index)\n",
    "    \n",
    "pred = out.float().cpu() > 0\n",
    "pred = utils.a_third_law(labels,pred)\n",
    "\n",
    "utils.get_results('./results/gat_pred.csv',pred, X_test_df)  # SAVING RESULTS IN A .CSV "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oooooooo... 0.5207!!! This is the best score obtained until now but it is drammatically low again. How is possible to obtain at least the same F1 score of which the papers talking about? We are wary so we compare our codes and dataset with their. It turns out that our dataset is really similar, but different. Each dimension is ugual, also the labels matrix and adjacency matrix are the same; however the features are different: instead to be binary, their features matrix  $\\in [0,1]^{50}$ (it's continuos instead discrete). Continuing with the comparison we infer that our features matrix is just their one activated with a non-liner function (probabily a simple step function). <br><br>\n",
    "Finally we can set our mind at rest: <b>our data is too less rappresentatitive to obtain a good accurancy</b> as the same algorithms can obtain with similar problems. This doent's mean that each embeddings that we have built until now is useless, conversely each one is a simpler structure where we store better the sparse informations of the dataset. However if the data are insufficient to multi-label vertex classification, also the best embedding can't add new informations.\n",
    "To show the little worth of our predictions, we show now an our simple statistical algorithm that obtain the same accurancy compute with F1 without consider any features and graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STUPID STATISTICAL APPROACH\n",
    "\n",
    "We want to obtain the best accuracy only through statistical considerations on the labels' distributions. Just with these few informations we can't predict if a new vertex has more or less probability than the others vertexes to have one in a single element of its label. We exclude to build randomly our prediction, so only two strategies remain: put one at the same class of each vertex, or put zero.<br>\n",
    "To understand the best way to combine this two strategies we build the vector $L_{mean}\\in [0,1]^{122}$ as follow:<br><br>\n",
    "$$ L_{mean}(i) = \\frac{\\mid\\{v\\mid L_{v,i}=1\\}\\mid}{n_{train}}$$ <br>\n",
    "$L_{mean}$ rapresents the mean of each label class in the training set. As we saw initially, for the Central Limit Theorem, each one of these values should tend to the mean of the same label class in the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmean = utils.get_lmean(labels)    \n",
    "lmean = utils.sort_lmean(lmean)  #  DESCENDING ORDER \n",
    "plt.plot(lmean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is to force one to a number $s$ of label classes $i$ if $L_{mean}(i)$ is 'big enough'. It remains to define what 'big enough' means. We reorder the element of $L_{mean}$ from the bigger to the smaller and we evaluete $V(s)$, an our approximation of $F1$ for this algorithm.\n",
    "\n",
    "$$V(s) = 2\\cdot \\frac {\\sum_{i=1}^{s} L_{mean}(i)}{\\sum_{i=1}^{122} L_{mean}(i) + n_{train}\\cdot s} \\simeq F1(s)$$\n",
    "\n",
    "The variable $s$  goes from $1$ to $122$ and expresses the number of the classes that we activate, starting from that one with bigger mean in the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = 0\n",
    "k = 0\n",
    "f1_values = np.zeros(lmean.shape[0])\n",
    "for i in range(lmean.shape[0]):\n",
    "    k += lmean[i][0]\n",
    "for s in range(lmean.shape[0]):\n",
    "    A += lmean[s][0]\n",
    "    f1 = 2 * A / (k+ntr*(s+1))\n",
    "    f1_values[s] = f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Valuating the function $V$ in all his domain $\\{1,2,...,122\\}$ and considering the point of its maximum we find the value of $s$ that maximize our strategy and an esteem of its relative F1 score:<br>\n",
    "$$s_{max}=57$$ <br>\n",
    "$$V(s_{max})=0.527$$ <br>\n",
    "The following plot rapresents the trend of $V(s)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.plot(f1_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = lmean_f[:np.argmax(f1_values),1]\n",
    "pred = np.zeros((X_test.shape[0],labels.shape[1]))\n",
    "\n",
    "for col in range(labels.shape[1]):\n",
    "    for i in range(np.argmax(f1_values)):\n",
    "        if(mask[i] == col):\n",
    "            pred[:,col] =1 \n",
    "            \n",
    "utils.get_results('./results/stat.csv',pred, X_test_df)  # SAVING RESULTS IN A .CSV  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluate this prediction also on the test set and we obtain an accuracy of $0.485$ in line with our prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This value is bigger than accuracy of GCN, Node2Vec and GraphSAGE and it is obtained without consider any features and graph, and with a costant prediction for each element of the test set. This result allow us to  emphasize a last important conlusion. <br>\n",
    "$$Machine\\space Learning\\space is\\space not\\space magic!$$\n",
    "If the data aren't enough, we can't expect certain performance also from the best machine learning algorithm for that specific task. Furthermore it is also possible that, in this cases, statistic alone performes better than state-of-the-art algorithm unable to train correctly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
